from crewai.flow.flow import Flow, listen, start
from pydantic import BaseModel
import logging
import os
import tempfile
from typing import Optional, Dict, Any, List

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Define a state model for structured state management
class DocumentProcessingState(BaseModel):
    bucket_name: str = ""
    object_key: str = ""
    local_path: str = ""
    file_name: str = ""
    category: str = ""
    confidence: float = 0.0
    reasoning: str = ""
    custom_category: Optional[str] = None
    target_key: Optional[str] = ""
    summary: Optional[str] = ""
    # ID field is auto-generated by Flow

class DocumentProcessFlow(Flow[DocumentProcessingState]):
    """CrewAI Flow for document processing"""
    
    def __init__(self, watsonx_model, aws_s3_client=None, ms_graph_client=None):
        """Initialize the document processing flow
        
        Args:
            watsonx_model: WatsonX model instance
            aws_s3_client: AWS S3 client for storage
            ms_graph_client: Microsoft Graph client for email
        """
        super().__init__()
        self.model = watsonx_model
        self.aws_s3_client = aws_s3_client
        self.ms_graph_client = ms_graph_client
        
        # Name and description for the flow
        self.name = "Document Processing Flow"
        self.description = "Process documents through classification, organization, and summarization"
    
    @start()
    def download_document(self):
        """Starting point: Download the document from S3"""
        logger.info(f"Starting flow with ID: {self.state.id}")
        logger.info(f"Downloading document {self.state.object_key} from bucket {self.state.bucket_name}")
        
        if not self.aws_s3_client:
            logger.error("AWS S3 client not initialized")
            return {"error": "AWS S3 client not initialized"}
        
        try:
            # Download the document
            local_path = self.aws_s3_client.download_file(
                self.state.bucket_name, 
                self.state.object_key
            )
            
            if not local_path:
                logger.error(f"Failed to download document: {self.state.object_key}")
                return {"error": f"Failed to download document: {self.state.object_key}"}
            
            # Update state with local path
            self.state.local_path = local_path
            self.state.file_name = os.path.basename(self.state.object_key)
            
            logger.info(f"Document downloaded to {local_path}")
            return {"local_path": local_path, "success": True}
        except Exception as e:
            logger.error(f"Error downloading document: {str(e)}")
            return {"error": str(e), "success": False}
    
    @listen(download_document)
    def classify_document(self, download_result):
        """Classify the document after it's downloaded"""
        if not download_result.get("success", False):
            logger.error("Document download failed, skipping classification")
            return download_result
        
        logger.info(f"Classifying document: {self.state.local_path}")
        
        try:
            # Import here to avoid circular imports
            from document_classifier import DocumentClassifier, DocumentCategory
            
            # Create classifier and classify document
            classifier = DocumentClassifier(self.model)
            category, confidence, reasoning, custom_category = classifier.classify_document(self.state.local_path)
            
            # Update state with classification result
            self.state.category = category.name if hasattr(category, "name") else str(category)
            self.state.confidence = confidence
            self.state.reasoning = reasoning
            self.state.custom_category = custom_category
            
            # Determine folder name
            if category == DocumentCategory.CUSTOM and custom_category:
                folder_name = custom_category
            else:
                folder_name = DocumentCategory.to_folder_name(category)
            
            classification_result = {
                "category": self.state.category,
                "confidence": confidence,
                "reasoning": reasoning,
                "custom_category": custom_category,
                "folder_name": folder_name,
                "success": True
            }
            
            logger.info(f"Document classified as {self.state.category} with confidence {confidence}")
            return classification_result
        except Exception as e:
            logger.error(f"Error classifying document: {str(e)}")
            return {"error": str(e), "success": False}
    
    @listen(classify_document)
    def organize_document(self, classification_result):
        """Organize the document based on its classification"""
        if not classification_result.get("success", False):
            logger.error("Document classification failed, skipping organization")
            return classification_result
        
        folder_name = classification_result.get("folder_name")
        logger.info(f"Organizing document into folder: {folder_name}")
        
        try:
            if not self.aws_s3_client:
                logger.error("AWS S3 client not initialized")
                return {"error": "AWS S3 client not initialized", "success": False}
            
            # Create folder marker
            folder_marker = f"{folder_name}/"
            self.aws_s3_client.s3_client.put_object(
                Bucket=self.state.bucket_name,
                Key=folder_marker,
                Body=''
            )
            logger.info(f"Created folder marker: {folder_marker}")
            
            # Determine target key
            base_name = os.path.basename(self.state.object_key)
            target_key = f"{folder_name}/{base_name}"
            
            # Copy the object
            copy_source = {'Bucket': self.state.bucket_name, 'Key': self.state.object_key}
            self.aws_s3_client.s3_client.copy_object(
                CopySource=copy_source,
                Bucket=self.state.bucket_name,
                Key=target_key
            )
            logger.info(f"Copied object to {target_key}")
            
            # Delete the original
            self.aws_s3_client.s3_client.delete_object(
                Bucket=self.state.bucket_name,
                Key=self.state.object_key
            )
            logger.info(f"Deleted original object at {self.state.object_key}")
            
            # Update state
            self.state.target_key = target_key
            
            organization_result = {
                "source_key": self.state.object_key,
                "target_key": target_key,
                "folder": folder_name,
                "success": True
            }
            
            return organization_result
        except Exception as e:
            logger.error(f"Error organizing document: {str(e)}")
            return {"error": str(e), "success": False}
    
    @listen(organize_document)
    def generate_summary(self, organization_result):
        """Generate a summary of the document"""
        if not organization_result.get("success", False):
            logger.warning("Document organization failed, but continuing with summary generation")
        
        logger.info(f"Generating summary for document: {self.state.local_path}")
        
        try:
            if not hasattr(self.model, 'generate_text'):
                logger.warning("Model does not support generate_text for summary generation")
                return {"summary": "Summary not available", "success": True}
            
            # Extract content from the document
            try:
                # First try using custom_pdf_tool if available
                from custom_pdf_tool import CustomPDFSearchTool
                pdf_tool = CustomPDFSearchTool(self.state.local_path, self.model)
                content = pdf_tool.search("main topics key points executive summary")
            except ImportError:
                try:
                    # Fall back to dockling_tool if available
                    from dockling_tool import DocklingPDFTool
                    pdf_tool = DocklingPDFTool(self.state.local_path, self.model)
                    content = pdf_tool.search("main topics key points executive summary")
                except ImportError:
                    # If neither tool is available, just use a generic message
                    content = "Please summarize this document."
            
            # Generate summary
            summary_prompt = f"""
            Based on the following document content, provide a comprehensive summary:
            
            {content}
            
            Focus on key points, main findings, and important details.
            """
            
            summary = self.model.generate_text(summary_prompt)
            self.state.summary = summary
            
            logger.info("Summary generated successfully")
            return {
                "summary": summary,
                "classification": self.state.category,
                "confidence": self.state.confidence,
                "organization": {"target_key": self.state.target_key},
                "success": True
            }
        except Exception as e:
            logger.error(f"Error generating summary: {str(e)}")
            return {"error": str(e), "success": False}

# Class for integrating with existing application
class DocumentProcessingFlow:
    """Manager class for document processing flows"""
    
    def __init__(self, watsonx_model, aws_s3_client=None, ms_graph_client=None):
        """Initialize the document processing manager
        
        Args:
            watsonx_model: WatsonX model instance
            aws_s3_client: AWS S3 client for storage
            ms_graph_client: Microsoft Graph client for email
        """
        self.model = watsonx_model
        self.aws_s3_client = aws_s3_client
        self.ms_graph_client = ms_graph_client
    
    def process_document(self, bucket_name, object_key):
        """Process a document using the CrewAI Flow
        
        Args:
            bucket_name (str): S3 bucket name
            object_key (str): S3 object key
            
        Returns:
            dict: Processing results
        """
        try:
            # Create a new flow instance for this document
            flow = DocumentProcessFlow(
                self.model,
                self.aws_s3_client,
                self.ms_graph_client
            )
            
            # Initialize state with document info
            flow.state.bucket_name = bucket_name
            flow.state.object_key = object_key
            
            # Start the flow
            logger.info(f"Starting flow for document {object_key} in bucket {bucket_name}")
            result = flow.kickoff()
            
            if result.get("success", False):
                return {
                    "success": True,
                    "message": f"Document processed successfully: {object_key}",
                    "results": {
                        "classification": {
                            "category": flow.state.category,
                            "confidence": flow.state.confidence,
                            "reasoning": flow.state.reasoning,
                            "custom_category": flow.state.custom_category
                        },
                        "organization": {
                            "target_key": flow.state.target_key
                        },
                        "summary": flow.state.summary
                    }
                }
            else:
                return {
                    "success": False,
                    "error": result.get("error", "Unknown error"),
                    "message": f"Failed to process document: {object_key}"
                }
        except Exception as e:
            logger.error(f"Error in process_document: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            return {"success": False, "error": str(e)}
    
    def scan_bucket(self, bucket_name):
        """Scan a bucket for documents to process
        
        Args:
            bucket_name (str): S3 bucket name
            
        Returns:
            dict: Results of the scan operation
        """
        # Implement the same bucket scanning logic as before
        # ...
        
    def scan_all_buckets(self):
        """Scan all monitored buckets for documents
        
        Returns:
            dict: Results from all bucket scans
        """
        # Implement the same all bucket scanning logic as before
        # ...

# Integration function for Streamlit
def integrate_document_flow():
    """Create function to integrate with your existing Streamlit app"""
    import streamlit as st
    
    # Initialize the document flow manager if not already in session state
    if 'document_flow' not in st.session_state:
        # First check if pdf_agent exists and has a model attribute
        if 'pdf_agent' in st.session_state and st.session_state.pdf_agent is not None and hasattr(st.session_state.pdf_agent, 'model'):
            watsonx_model = st.session_state.pdf_agent.model
            aws_s3_client = st.session_state.aws_s3_client if 'aws_s3_client' in st.session_state else None
            ms_graph_client = st.session_state.ms_graph_client if 'ms_graph_client' in st.session_state else None
            
            try:
                # Create the document processing manager
                st.session_state.document_flow = DocumentProcessingFlow(
                    watsonx_model,
                    aws_s3_client,
                    ms_graph_client
                )
                st.success("Autonomous document processing flow initialized!")
            except Exception as e:
                st.error(f"Error initializing document flow: {str(e)}")
                try:
                    from document_classifier import DocumentClassifier
                    st.session_state.classifier = DocumentClassifier(watsonx_model)
                    st.success("Document classifier initialized (simplified mode)")
                except Exception as e2:
                    st.error(f"Could not initialize simplified classifier: {str(e2)}")
        else:
            st.error("Cannot initialize document flow: WatsonX model not properly initialized. Please initialize the WatsonX model first.")